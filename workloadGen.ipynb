{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数：a 类型错误 应该为：<class 'int'>\n",
      "参数：b 类型错误 应该为：<class 'int'>\n",
      "helloworld\n"
     ]
    }
   ],
   "source": [
    "from typing import get_type_hints\n",
    "from functools import wraps\n",
    "from inspect import getfullargspec\n",
    " \n",
    "# 定义函数参数类型的检查函数\n",
    "def parameter_check(obj, **kwargs):\n",
    "    hints = get_type_hints(obj)\n",
    "    for label_name, label_type in hints.items():\n",
    "        # print(label_name)\n",
    "        # print(label_type)\n",
    "        # 返回类型不检查 跳过 只检查实际传入参数的类型是否正确\n",
    "        if label_name == \"return\":\n",
    "            continue\n",
    "        # 判断实际传入的参数是否与函数标签中的参数一致\n",
    "        if not isinstance(kwargs[label_name], label_type):\n",
    "            print(f\"参数：{label_name} 类型错误 应该为：{label_type}\")\n",
    "\n",
    "# 使用装饰器进行函数包裹\n",
    "def wrapped_func(decorator):\n",
    "    @wraps(decorator)\n",
    "    def wrapped_decorator(*args, **kwargs):\n",
    "        func_args = getfullargspec(decorator)[0]\n",
    "        kwargs.update(dict(zip(func_args, args)))\n",
    "        parameter_check(decorator, **kwargs)\n",
    "        return decorator(**kwargs)\n",
    "    return wrapped_decorator\n",
    "\n",
    "@wrapped_func\n",
    "def add0(a: int, b: int) -> int:\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@wrapped_func\n",
    "def add1(a: int, b: float = 520.1314) -> float:\n",
    "    return a + b\n",
    "\n",
    "# print(add0(1, 1))\n",
    "print(add0(\"hello\", \"world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "# new one\n",
    "class simpleSQL:\n",
    "    def __init__(self) -> None:\n",
    "        self.token=[]\n",
    "        \n",
    "    # def __init__(self,token = None) -> None:\n",
    "    #     self.token=token\n",
    "        \n",
    "    def add(self,x):\n",
    "        self.token.append(x)\n",
    "    \n",
    "    def toStr(self):\n",
    "        ans=\"\"\n",
    "        for i in range(len(self.token)):\n",
    "            ans+=str(self.token[i].value)\n",
    "            if i!=len(self.token)-1 \\\n",
    "                and self.token[i].type!=\"tbname_\"\\\n",
    "                    and self.token[i].type!=\"dot\"\\\n",
    "                        and self.token[i].type!=\"colname_\":\n",
    "                ans+=\" \"\n",
    "            # print(str(self.token[i].value)+\" \",end=\"\")\n",
    "        # print()\n",
    "        return ans+\"\\n\"\n",
    "\n",
    "class key:\n",
    "    def __init__(self,value,type) -> None:\n",
    "        self.value=value\n",
    "        self.type=type\n",
    "        # self.name=name\n",
    "        \n",
    "    def toStr(self):\n",
    "        print(f'value : {self.value}')\n",
    "        print(f'type : {self.type}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class foreign_constraint:\n",
    "    def __init__(self,tb1,col1,tb2,col2) -> None:\n",
    "        self.tb1=tb1\n",
    "        self.col1=col1\n",
    "        self.tb2=tb2\n",
    "        self.col2=col2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class column:\n",
    "    def __init__(self,type,name,father) -> None:\n",
    "        self.name=name\n",
    "        self.data_type=type\n",
    "        self.father_table=father"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Table:\n",
    "    def __init__(self,tb_name,col,prim_col,foreign_constraint,column_distribution) -> None:\n",
    "        self.name=tb_name\n",
    "        self.col=col\n",
    "        self.col_data_dis={}\n",
    "        self.prim_col = prim_col\n",
    "        self.foreign_constraint = foreign_constraint\n",
    "        self.column_distribution = column_distribution\n",
    "    \n",
    "    def addCharacteristics(self,col_name,data_dis):\n",
    "        col_name_set=set(self.col[0:len(self.col)])\n",
    "        if col_name not in col_name_set:\n",
    "            print(\"error: add data characteristics failed. Col name not found.\")\n",
    "        else:\n",
    "            self.col_data_dis[col_name]=data_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBschema:\n",
    "    def __init__(self,tbs,foreign_constraint=None) -> None:\n",
    "        self.tables=tbs\n",
    "        # self.tbNum=len(tbs)\n",
    "        self.foreign_constraint=foreign_constraint\n",
    "    \n",
    "    def toStr(self):\n",
    "        ans=\"\"\n",
    "        for i,j in enumerate(self.tables):\n",
    "            ans+=\"table \"+str(i+1)\n",
    "            ans+=\" : \"+j.name\n",
    "            ans+=\"\\n\"\n",
    "        return ans\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Random sampling in a given probability distribution graph\n",
    "# ---- return the position(i.e. l+index) corresponding to the sample num.\n",
    "def rand_num_sampling(l,r,pdg):\n",
    "    if r-l+1 != len(pdg):\n",
    "        print(\"l is \"+str(l)+\" r is \"+str(r)+\" and pdg is \"+str(pdg))\n",
    "        print(\"error : pdg not match [l,r]\")\n",
    "        return l-1\n",
    "    one_=sum(pdg)\n",
    "    if abs(one_-1.0)>1e-5:\n",
    "        print(\"error : probability sum \"+str(one_)+\" do not equal 1\")\n",
    "        return l-1\n",
    "    pdg_=np.array(pdg)\n",
    "    pdg_=[round(i*1000) for i in pdg_]\n",
    "    maxv=sum(pdg_)\n",
    "    num=random.randint(0,maxv-1)\n",
    "    index=0\n",
    "    for i in range(len(pdg_)):\n",
    "        num=num-pdg_[i]\n",
    "        if num<0:\n",
    "            index=i\n",
    "            break\n",
    "    return l+index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_str_sampling(str_len=16):\n",
    "  random_str =''\n",
    "  base_str ='ABCDEFGHIGKLMNOPQRSTUVWXYZabcdefghigklmnopqrstuvwxyz0123456789'\n",
    "  length =len(base_str) -1\n",
    "  for i in range(str_len):\n",
    "    random_str +=base_str[random.randint(0, length)]\n",
    "  return random_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9963\n",
      "19920\n",
      "30062\n",
      "40054\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dict={}\n",
    "for i in range(1,6):\n",
    "    dict[i]=0\n",
    "for i in range(1,100000,1):\n",
    "    # print(randNumGen(1,5,np.full((5),0.2)))\n",
    "    # dict[randNumGen(1,5,np.full((5),0.2))]+=1\n",
    "    dict[rand_num_sampling(1,5,[0.1,0.2,0.3,0.4,0.0])]+=1\n",
    "for i in range(1,6):\n",
    "    print(dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681\n",
      "319\n"
     ]
    }
   ],
   "source": [
    "dict={}\n",
    "dict[0]=0\n",
    "dict[1]=0\n",
    "for i in range(1000):\n",
    "    dict[rand_num_sampling(0,1,np.array([0.7,0.3]))]+=1\n",
    "print(dict[0])\n",
    "print(dict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature1  : size of workload\n",
    "# feature2  : read/write ratio\n",
    "# feature3  : average table num\n",
    "# feature4  : table data distribution\n",
    "    # tb_choice=self.dbs.tables[randNumGen(0,len(self.dbs.tables)-1,condition.avgtb)]\n",
    "    # tb_choice=get1Table(self.dbs.tables,np.full(len(self.dbs.tables),1.0/len(self.dbs.tables)))\n",
    "# feature5  : different query comparison constraint ratio\n",
    "    # cons=randNumGen(0,3,condition.feature4)\n",
    "# feature6  : query data domain distribution\n",
    "    # data=randNumGen(0,99,[1.0/100 for i in range(100)])\n",
    "# feature7  : query logic predicate num\n",
    "# feature8  : average aggregation operator num\n",
    "# feature9  : average query colomn num\n",
    "# feature10 : group by ratio\n",
    "# feature11 : if ordered, desc or asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data access distribution\n",
    "class feature:\n",
    "    def __init__(self,workloadSize,rwRatio,averageTableNum,tableDistribution,\\\n",
    "        queryComparisonOperatorRatio,tableDomainDist,queryLogicPredicateNum,\\\n",
    "            averageAggregationOperatorNum,averageQueryColomnNum,groupByRatio,descOrAsc) -> None:\n",
    "        self.workloadSize=workloadSize # feature1\n",
    "        self.rwRatio=rwRatio # feature2\n",
    "        self.averageTableNum=averageTableNum # feature3\n",
    "        self.tableDistribution=tableDistribution # feature4\n",
    "        self.queryComparisonOperatorRatio = queryComparisonOperatorRatio # feature5\n",
    "        self.tableDomainDist = tableDomainDist # feature6\n",
    "        self.queryLogicPredicateNum = queryLogicPredicateNum # feature7\n",
    "        self.averageAggregationOperatorNum=averageAggregationOperatorNum # feature8\n",
    "        self.averageQueryColomnNum=averageQueryColomnNum # feature9\n",
    "        self.groupByRatio=groupByRatio # feature10\n",
    "        self.descOrAsc=descOrAsc # feature11"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### update table_name Set column_name = new_value Where column_name = some_value\n",
    "> ##### insert into table_name [(column1,column2,…)] value (value1,value2,…)\n",
    "> ##### select {col_name} from table_name where col_name <=> constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLGen:\n",
    "    def __init__(self,dbs) -> None:\n",
    "        self.sql_set=[]\n",
    "        self.last_sql=simpleSQL()\n",
    "        self.dbs=dbs\n",
    "\n",
    "    def generate(self,condition):\n",
    "        # feature1 : read/write ratio\n",
    "        rw_choice=rand_num_sampling(0,1,np.array([condition.rwRatio,1-condition.rwRatio]))\n",
    "        # feature2 : average table num\n",
    "        tb_num=rand_num_sampling(1,2,condition.averageTableNum)\n",
    "        # feature3 : table data distribution\n",
    "        # tb_choice=self.dbs.tables[randNumGen(0,len(self.dbs.tables)-1,[1.0/len(self.dbs.tables) for i in range(len(self.dbs.tables))])]\n",
    "        # print('aaaa')\n",
    "        tb_choice=self.dbs.tables[rand_num_sampling(0,len(self.dbs.tables)-1,condition.tableDistribution)]\n",
    "        # feature4 : different query comparison constraint ratio\n",
    "        # cons=randNumGen(0,3,[1.0/4 for i in range(4)])\n",
    "        cons=rand_num_sampling(0,3,condition.queryComparisonOperatorRatio)\n",
    "        # feature5 : query data domain distribution\n",
    "        data=rand_num_sampling(0,99,[1.0/100 for i in range(100)])\n",
    "        if rw_choice==0:\n",
    "            self.last_sql.add(key(value=\"select\",type=\"keyword\"))\n",
    "            self.last_sql.add(key(value=\"*\",type=\"colname\"))\n",
    "            self.last_sql.add(key(value=\"from\",type=\"keyword\"))\n",
    "            self.last_sql.add(key(value=tb_choice.name,type=\"tbname\"))\n",
    "            self.last_sql.add(key(value=\"where\",type=\"keyword\"))\n",
    "            self.last_sql.add(key(value=tb_choice.col[0].value,type=\"colname\"))\n",
    "            if cons==0:\n",
    "                self.last_sql.add(key(value=\">\",type=\"compare\"))\n",
    "            else:\n",
    "                if cons==1:\n",
    "                    self.last_sql.add(key(value=\"<\",type=\"compare\"))\n",
    "                else:\n",
    "                    if cons==2:\n",
    "                        self.last_sql.add(key(value=\"=\",type=\"compare\"))\n",
    "                    else: \n",
    "                        if cons==3:\n",
    "                            self.last_sql.add(key(value=\"!=\",type=\"compare\"))\n",
    "                        else: \n",
    "                            pass\n",
    "            self.last_sql.add(key(value=data,type=\"value\"))\n",
    "            self.last_sql.add(key(value=\";\",type=\"end\"))\n",
    "        else:\n",
    "            self.last_sql.add(key(value=\"insert\",type=\"keyword\"))\n",
    "            self.last_sql.add(key(value=\"into\",type=\"keyword\"))\n",
    "            self.last_sql.add(key(value=tb_choice.name,type=\"tbname\"))\n",
    "            self.last_sql.add(key(value=\"value\",type=\"keyword\"))\n",
    "            tmp=\"(\"\n",
    "            for i in range(len(tb_choice.col)):\n",
    "                if i!=0:\n",
    "                    tmp+=\",\"\n",
    "                tmp+=str(rand_num_sampling(0,99,[1.0/100 for i in range(100)]));\n",
    "            tmp+=\")\"\n",
    "            self.last_sql.add(key(value=tmp,type=\"value\"))\n",
    "            self.last_sql.add(key(value=\";\",type=\"end\"))\n",
    "        \n",
    "        self.sql_set.append(self.last_sql)\n",
    "        return self.last_sql\n",
    "\n",
    "    def save(self,outputFile):\n",
    "        with open(outputFile, 'w') as f:\n",
    "            for it in self.sql_set:\n",
    "                f.write(it.toStr())\n",
    "        print(f\"workload saved to {outputFile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filePath,mode):\n",
    "    jsonFile = open(filePath,mode)\n",
    "    input = json.loads(jsonFile.read())\n",
    "    # print(input)\n",
    "    all_tables = []\n",
    "    foreign_cons = []\n",
    "    cons = []\n",
    "    # load table name & column name\n",
    "    for table in input['Tables']:\n",
    "        tb_name = table['Table Name']\n",
    "        tb_col_distribution = table['Column Distribution']\n",
    "        tb_cols = []\n",
    "        for col in table['Table Columns']:\n",
    "            col_name = col['Column Name']\n",
    "            col_type = col['Data Type']\n",
    "            tb_cols.append(key(col_name,col_type))\n",
    "        \n",
    "        prim_key = key(table['Primary Key']['Name'],table['Primary Key']['Data Type'])\n",
    "        for con in table['Foreign Key']:\n",
    "            foreign_cons.append(foreign_constraint(tb_name,key(con['Foreign Key Name'],con['Foreign Key Type']),\n",
    "                                                con['Referenced Table'],key(con['Referenced Primary Key'],con['Referenced Primary Key Type'])))\n",
    "        all_tables.append(Table(tb_name,tb_cols,prim_key,foreign_cons,tb_col_distribution))\n",
    "    # load constraints\n",
    "    for con in input['Constraints']:\n",
    "        cons.append(input['Constraints'][con])\n",
    "    # print(cons)\n",
    "    fea = feature(cons[0],cons[1],cons[2],cons[3],cons[4],cons[5],cons[6],cons[7],cons[8],cons[9],cons[10])\n",
    "    #print(cons[0],cons[1],cons[2],cons[3],cons[4])\n",
    "    dbs=DBschema(tbs=all_tables,foreign_constraint=foreign_cons)\n",
    "    outputFile=input['Generation File']\n",
    "    return dbs,fea,outputFile\n",
    "\n",
    "# cons应该是feature类型\n",
    "# fea=feature(cons[0],cons[1])\n",
    "# return DBschema,feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tables,fea,outputFile=load_json('./input.json','r')\n",
    "table2dist = {}\n",
    "for i in range(len(all_tables.tables)):\n",
    "    table2dist[all_tables.tables[i]] = fea.tableDistribution[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from tb22 where col3 = 79 ;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tb=Table(\"lzz\",[key(\"age\",int),key(\"weight\",int)])\n",
    "dbs=all_tables\n",
    "sg=SQLGen(dbs=dbs)\n",
    "sg.generate(condition=fea)\n",
    "print(sg.last_sql.toStr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNTable(dbs,n,pdg):\n",
    "    # print(n)\n",
    "    all_tb=[dbs.tables[i] for i in range(len(dbs.tables))]\n",
    "    # all_tb=[i for i in len(dbs.tables)]\n",
    "    tb_set=set()\n",
    "    tb_selected = []\n",
    "    tb_sub_distribution = []\n",
    "    if len(all_tb)<n:\n",
    "        print(\"fatal error : dbs table num not enough.\")\n",
    "        return -1,set()\n",
    "    # possibly sample to the same table\n",
    "    while len(tb_set)<n:\n",
    "        x=rand_num_sampling(0,len(all_tb)-1,pdg)\n",
    "        if x!=-1:\n",
    "            tb_set.add(dbs.tables[x])\n",
    "            if dbs.tables[x] not in tb_selected:\n",
    "                tb_selected.append(dbs.tables[x])\n",
    "                tb_sub_distribution.append(table2dist[dbs.tables[x]])\n",
    "            else:\n",
    "                continue\n",
    "            # print(str(len(tb_set))+\"add\"+str(fea.tableDistribution[x]))\n",
    "        else:\n",
    "            return -1,set()\n",
    "    # make tb_sub_distribution sum to 1\n",
    "    sum_of_tsd = 0\n",
    "    for i in tb_sub_distribution:\n",
    "        sum_of_tsd = sum_of_tsd + i\n",
    "    # print(tb_sub_distribution)\n",
    "    for i in range(len(tb_sub_distribution)):\n",
    "        tb_sub_distribution[i] = tb_sub_distribution[i]/sum_of_tsd\n",
    "    # print(tb_sub_distribution)\n",
    "    return 1,tb_set,tb_sub_distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test of getNTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n"
     ]
    }
   ],
   "source": [
    "bool,tb_set,tb_dist_set=getNTable(dbs=dbs,n=3,pdg=[0.1,0.3,0.6,0.0])\n",
    "print(tb_dist_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2864\n",
      "7865\n",
      "9271\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dict={}\n",
    "for i in range(len(dbs.tables)):\n",
    "    dict[dbs.tables[i].name]=0\n",
    "for i in range(10000):\n",
    "    bool,tb_set,tb_dist_set=getNTable(dbs=dbs,n=2,pdg=[0.1,0.3,0.6,0.0])\n",
    "    for i in tb_set:\n",
    "        # print(i.name)\n",
    "        dict[i.name]+=1\n",
    "for i in range(len(dbs.tables)):\n",
    "    print(dict[dbs.tables[i].name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get1Table(tbs,pdg):\n",
    "    # print(\"get1\"+str(len(tbs)-1))\n",
    "    return dbs.tables[rand_num_sampling(0,len(tbs)-1,pdg)]\n",
    "\n",
    "def get1Column(tb,pdg):\n",
    "    return tb.col[rand_num_sampling(0,len(tb.col)-1,pdg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4, 0.4, 0.2]\n",
      "[0.25, 0.25, 0.25, 0.25]\n",
      "[0.5, 0.3, 0.2]\n"
     ]
    }
   ],
   "source": [
    "print(fea.queryLogicPredicateNum)\n",
    "print(fea.tableDistribution)\n",
    "print(fea.averageAggregationOperatorNum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### select [max/avg/sum(col_name) as as_name] from table_name where ...\n",
    "##### select * from table_name where ... group by col_name\n",
    "> ##### count/group-by usually involves discrete variables\n",
    "> ##### sum/max/min/avg usually involves continuious variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_tb_get_continuous_type_column(tbs):\n",
    "    pass\n",
    "    \n",
    "def from_tb_get_discrete_type_column(tbs):\n",
    "    pass\n",
    "\n",
    "def from_tb_get_N_columns_for_aggregation(tbs,aggregationNum):\n",
    "    \n",
    "    col_set=set()\n",
    "    while(len(col_set)<aggregationNum):\n",
    "        tb_choice=tbs[rand_num_sampling(0,len(tbs)-1,np.full(len(tbs),1.0/len(tbs)))]\n",
    "        col_choice=get1Column(tb_choice,np.full(len(tb_choice.col),1.0/len(tb_choice.col)))\n",
    "        col_set.add(key(value=col_choice,type=tb_choice))\n",
    "        # pass\n",
    "    return list(col_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLGen2(SQLGen):\n",
    "    def generate(self,condition):\n",
    "        self.last_sql=simpleSQL()\n",
    "        rw_choice=rand_num_sampling(0,1,np.array([condition.rwRatio,1-condition.rwRatio]))\n",
    "        if rw_choice==0:\n",
    "            # read\n",
    "            tb_num=rand_num_sampling(1,2,condition.averageTableNum)\n",
    "            \n",
    "            if tb_num==1:\n",
    "                tb_choice=get1Table(self.dbs.tables,condition.tableDistribution)\n",
    "                col_choice=get1Column(tb_choice,tb_choice.column_distribution)\n",
    "                \n",
    "                self.last_sql.add(key(value=\"select\",type=\"keyword\"))\n",
    "                \n",
    "                on=rand_num_sampling(0,2,condition.averageAggregationOperatorNum)\n",
    "                aqcn=min(len(tb_choice.col),len(condition.averageQueryColomnNum)-1)\n",
    "                qcn=rand_num_sampling(0,aqcn,[1.0/(aqcn+1) for i in range(aqcn+1)])\n",
    "                \n",
    "                if qcn==0:\n",
    "                    self.last_sql.add(key(value=\"*\",type=\"colname\"))\n",
    "                else:\n",
    "                    if col_choice.type=='varchar':\n",
    "                        aggregation_type=0\n",
    "                    else:\n",
    "                        aggregation_type=rand_num_sampling(1,3,[1.0/3 for i in range(3)])\n",
    "                        \n",
    "                    if aggregation_type==0:\n",
    "                        tmp=f'count({col_choice.value}) as count_value_{col_choice.value}'\n",
    "                    elif aggregation_type==1:\n",
    "                        tmp=f'avg({col_choice.value}) as average_value_{col_choice.value}'\n",
    "                    elif aggregation_type==2:\n",
    "                        tmp=f'min({col_choice.value}) as minimum_value_{col_choice.value}'\n",
    "                    elif aggregation_type==3:\n",
    "                        tmp=f'max({col_choice.value}) as maximum_value_{col_choice.value}'\n",
    "                                \n",
    "                    self.last_sql.add(key(value=tmp,type=\"aggregation\"))\n",
    "                    \n",
    "                self.last_sql.add(key(value=\"from\",type=\"keyword\"))\n",
    "                self.last_sql.add(key(value=tb_choice.name,type=\"tbname\"))\n",
    "                \n",
    "                hasGroupBy=rand_num_sampling(0,1,condition.groupByRatio)\n",
    "                # print(\"groupby\")\n",
    "                tb_choice=get1Table(self.dbs.tables,condition.tableDistribution)\n",
    "                col_choice=get1Column(tb_choice,tb_choice.column_distribution)\n",
    "                \n",
    "                if hasGroupBy==0 or col_choice.type!=\"varchar\":\n",
    "                    \n",
    "                    self.last_sql.add(key(value=\"where\",type=\"keyword\"))\n",
    "                    \n",
    "                    qc_num=rand_num_sampling(1,3,condition.queryLogicPredicateNum)\n",
    "                    for i in range(qc_num):\n",
    "                        if i!=0:\n",
    "                            if rand_num_sampling(0,1,[0.5,0.5])==0:\n",
    "                                self.last_sql.add(key(value=\"and\",type=\"predicate\"))\n",
    "                            else:\n",
    "                                self.last_sql.add(key(value=\"or\",type=\"predicate\"))\n",
    "                        \n",
    "                        # tb_choice=get1Table(self.dbs,np.full(len(self.dbs.tables),1.0/len(self.dbs.tables)))\n",
    "                        # col_choice=get1Column(tb_choice,np.full(len(tb_choice.col),1.0/len(tb_choice.col)))\n",
    "                        col_choice=get1Column(tb_choice,tb_choice.column_distribution)\n",
    "\n",
    "                        self.last_sql.add(key(value=tb_choice.name,type=\"tbname_\"))\n",
    "                        self.last_sql.add(key(value=\".\",type=\"dot\"))\n",
    "                        self.last_sql.add(key(value=col_choice.value,type=\"colname\"))\n",
    "                        \n",
    "                        cons=rand_num_sampling(0,3,condition.queryComparisonOperatorRatio)\n",
    "                        if cons==0:\n",
    "                            self.last_sql.add(key(value=\">\",type=\"compare\"))\n",
    "                        elif cons==1:\n",
    "                            self.last_sql.add(key(value=\"<\",type=\"compare\"))\n",
    "                        elif cons==2:\n",
    "                            self.last_sql.add(key(value=\"=\",type=\"compare\"))\n",
    "                        elif cons==3:\n",
    "                            self.last_sql.add(key(value=\"!=\",type=\"compare\"))\n",
    "                        else:\n",
    "                            pass\n",
    "                        data=rand_num_sampling(0,99,[1.0/100 for i in range(100)])\n",
    "                        self.last_sql.add(key(value=data,type=\"value\"))\n",
    "                    \n",
    "                    # self.sql.add(key(value=\";\",type=\"end\"))\n",
    "                else:\n",
    "                    self.last_sql.add(key(value=\"group by\",type=\"keyword\"))\n",
    "                    self.last_sql.add(key(value=col_choice.value,type=\"colname\"))\n",
    "                \n",
    "                hasOrderBy=rand_num_sampling(0,1,condition.descOrAsc)\n",
    "                descOrAsc=rand_num_sampling(0,1,[0.5,0.5])\n",
    "                col_choice=get1Column(tb_choice,tb_choice.column_distribution)\n",
    "                if hasOrderBy==1 and col_choice.type!=\"varchar\" and descOrAsc==0:\n",
    "                    self.last_sql.add(key(value=\"order by\",type=\"keyword\"))\n",
    "                    self.last_sql.add(key(value=col_choice.value,type=\"colname\"))\n",
    "                    self.last_sql.add(key(value=\"desc\",type=\"sort\"))\n",
    "                elif hasOrderBy==1 and col_choice.type!=\"varchar\":\n",
    "                    self.last_sql.add(key(value=\"order by\",type=\"keyword\"))\n",
    "                    self.last_sql.add(key(value=col_choice.value,type=\"colname\"))\n",
    "                    self.last_sql.add(key(value=\"asc\",type=\"sort\"))\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            elif tb_num==2:\n",
    "                # JOIN\n",
    "                bool,tbs,tbs_sub_distribution=getNTable(dbs=dbs,n=2,pdg=np.full(len(dbs.tables),1.0/len(dbs.tables)))\n",
    "                tbs_=list(tbs)\n",
    "                if bool==-1:\n",
    "                    print(\"fatal error : dbs sampled failed.\")\n",
    "                else:\n",
    "                    self.last_sql.add(key(value=\"select\",type=\"keyword\"))\n",
    "                    \n",
    "                    on=rand_num_sampling(0,2,condition.averageAggregationOperatorNum)\n",
    "                    aqcn=len(condition.averageQueryColomnNum)-1\n",
    "                    qcn=rand_num_sampling(0,aqcn,[1.0/(aqcn+1) for i in range(aqcn+1)])\n",
    "                    \n",
    "                    if qcn==0:\n",
    "                        self.last_sql.add(key(value=\"*\",type=\"colname\"))\n",
    "                    else:\n",
    "                        res=from_tb_get_N_columns_for_aggregation(tbs_,qcn)\n",
    "                        n_col=[res[i].value for i in range(len(res))]\n",
    "                        n_col_from_tb=[res[i].type for i in range(len(res))]\n",
    "                        for i,col in enumerate(n_col):\n",
    "                            if i<on:\n",
    "                                if col.type=='varchar':\n",
    "                                    aggregation_type=0\n",
    "                                else:\n",
    "                                    aggregation_type=rand_num_sampling(1,3,[1.0/3 for i in range(3)])  \n",
    "                                if aggregation_type==0:\n",
    "                                    tmp=f'count({n_col_from_tb[i].name}.{col.value}) as count_value_{col.value}'\n",
    "                                elif aggregation_type==1:\n",
    "                                    tmp=f'avg({n_col_from_tb[i].name}.{col.value}) as average_value_{col.value}'\n",
    "                                elif aggregation_type==2:\n",
    "                                    tmp=f'min({n_col_from_tb[i].name}.{col.value}) as minimum_value_{col.value}'\n",
    "                                elif aggregation_type==3:\n",
    "                                    tmp=f'max({n_col_from_tb[i].name}.{col.value}) as maximum_value_{col.value}'\n",
    "                                \n",
    "                                if i!=qcn-1:\n",
    "                                    self.last_sql.add(key(value=tmp,type=\"colname_\"))\n",
    "                                else:\n",
    "                                    self.last_sql.add(key(value=tmp,type=\"colname\"))\n",
    "                                \n",
    "                            else:\n",
    "                                if i!=qcn-1:\n",
    "                                    self.last_sql.add(key(value=f\"{n_col_from_tb[i].name}.{col.value}\",type=\"colname_\"))\n",
    "                                else:\n",
    "                                    self.last_sql.add(key(value=f\"{n_col_from_tb[i].name}.{col.value}\",type=\"colname\"))\n",
    "                            # print(i.value)\n",
    "                            if i!=qcn-1:\n",
    "                                self.last_sql.add(key(value=\",\",type=\"dot\"))\n",
    "\n",
    "                    self.last_sql.add(key(value=\"from\",type=\"keyword\"))\n",
    "\n",
    "                    self.last_sql.add(key(value=tbs_[0].name,type=\"tbname\"))\n",
    "                    self.last_sql.add(key(value=\"join\",type=\"keyword\"))\n",
    "                    self.last_sql.add(key(value=tbs_[1].name,type=\"tbname\"))\n",
    "                    self.last_sql.add(key(value=\"where\",type=\"keyword\"))\n",
    "                    \n",
    "                    self.last_sql.add(key(value=tbs_[0].name,type=\"tbname_\"))\n",
    "                    self.last_sql.add(key(value=\".\",type=\"dot\"))\n",
    "                    self.last_sql.add(key(value=tbs_[0].col[0].value,type=\"colname\"))\n",
    "                    \n",
    "                    self.last_sql.add(key(value=\"=\",type=\"compare\"))\n",
    "                    \n",
    "                    self.last_sql.add(key(value=tbs_[1].name,type=\"tbname_\"))\n",
    "                    self.last_sql.add(key(value=\".\",type=\"dot\"))\n",
    "                    self.last_sql.add(key(value=tbs_[1].col[0].value,type=\"colname\"))\n",
    "                    \n",
    "                    qc_num=rand_num_sampling(1,3,condition.queryLogicPredicateNum)\n",
    "                    if qc_num>1:\n",
    "                        \n",
    "                        for i in range(qc_num-1):\n",
    "                            if rand_num_sampling(0,1,[0.5,0.5])==0:\n",
    "                                self.last_sql.add(key(value=\"and\",type=\"predicate\"))\n",
    "                            else:\n",
    "                                self.last_sql.add(key(value=\"or\",type=\"predicate\"))\n",
    "                            \n",
    "                            tb_choice=get1Table(tbs_,tbs_sub_distribution)\n",
    "                            col_choice=get1Column(tb_choice,tb_choice.column_distribution)\n",
    "                            \n",
    "                            self.last_sql.add(key(value=tb_choice.name,type=\"tbname_\"))\n",
    "                            self.last_sql.add(key(value=\".\",type=\"dot\"))\n",
    "                            self.last_sql.add(key(value=col_choice.value,type=\"colname\"))\n",
    "                        \n",
    "                            cons=rand_num_sampling(0,3,condition.queryComparisonOperatorRatio)\n",
    "                            if cons==0:\n",
    "                                self.last_sql.add(key(value=\">\",type=\"compare\"))\n",
    "                            elif cons==1:\n",
    "                                self.last_sql.add(key(value=\"<\",type=\"compare\"))\n",
    "                            elif cons==2:\n",
    "                                self.last_sql.add(key(value=\"=\",type=\"compare\"))\n",
    "                            elif cons==3:\n",
    "                                self.last_sql.add(key(value=\"!=\",type=\"compare\"))\n",
    "                            else:\n",
    "                                pass\n",
    "                            data=rand_num_sampling(0,99,[1.0/100 for i in range(100)])\n",
    "                            self.last_sql.add(key(value=data,type=\"value\"))\n",
    "                        \n",
    "                            # self.sql.add(key(value=\";\",type=\"end\"))\n",
    "                    else:\n",
    "                        pass\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        elif rw_choice==1:\n",
    "            tb_choice=self.dbs.tables[rand_num_sampling(0,len(self.dbs.tables)-1,condition.tableDistribution)]\n",
    "            self.last_sql.add(key(value=\"insert\",type=\"keyword\"))\n",
    "            self.last_sql.add(key(value=\"into\",type=\"keyword\"))\n",
    "            self.last_sql.add(key(value=tb_choice.name,type=\"tbname\"))\n",
    "            self.last_sql.add(key(value=\"value\",type=\"keyword\"))\n",
    "            tmp=\"(\"\n",
    "            for i in range(len(tb_choice.col)):\n",
    "                if i!=0:\n",
    "                    tmp+=\",\"\n",
    "                if tb_choice.col[i].type=='varchar':\n",
    "                    tmp+=rand_str_sampling(5)\n",
    "                else:\n",
    "                    tmp+=str(rand_num_sampling(0,99,[1.0/100 for i in range(100)]));\n",
    "            tmp+=\")\"\n",
    "            self.last_sql.add(key(value=tmp,type=\"value\"))\n",
    "            # self.sql.add(key(value=\";\",type=\"end\"))\n",
    "        else:\n",
    "            pass\n",
    "        self.sql_set.append(self.last_sql)\n",
    "        return self.last_sql\n",
    "\n",
    "    def generate_N(self, condition, outputFile):\n",
    "        self.sql_set=[]\n",
    "        # ans=list()\n",
    "        for i in range(condition.workloadSize):\n",
    "            self.generate(condition)\n",
    "        \n",
    "        if condition.workloadSize==len(self.sql_set):\n",
    "            print(\"workload generation succeeded.\")\n",
    "            self.save(outputFile)\n",
    "        else:\n",
    "            print(\"fatal error : generation failed. Please check your schema input.\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workload generation succeeded.\n",
      "workload saved to workloadGen.wg\n"
     ]
    }
   ],
   "source": [
    "# all_tables,cons=loadJson('input.json','r')\n",
    "# dbs=DBschema(tbs=all_tables,cons=None)\n",
    "sg=SQLGen2(dbs=dbs)\n",
    "sg.generate(condition=fea)\n",
    "# print(sg.last_sql.toStr())\n",
    "sg.generate_N(condition=fea,outputFile=outputFile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9ac02aae607139adf8e5fa8a62e114f592a713bf43b6e1a58735c485260187e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
